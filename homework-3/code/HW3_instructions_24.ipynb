{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will preprocess the dataset and perform some basic regression and classification tasks. The learning outcome of this part is to know how one can pre-process a real-world dataset and perform a supervised learning task, and to understand some of the fundamental mechanisms behind these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student information\n",
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'aabb0000'\n",
    "STUD_NAME = 'Aaaa Bbbb'\n",
    "STUD_EMAIL = 'aaaa.bbbb@dsv.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Pass/Fail.\n",
    "\n",
    "To Pass this HW you need to provide a complete and correct solution, where one minor mistake is allowed. However, if your solution has more minor mistakes or lacks parts entirely or has one or more major mistakes, then you receive a Fail grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, regression task and classification task\n",
    "\n",
    "1. Reading the files\n",
    "2. Missing Values\n",
    "3. Imputing categorical variables\n",
    "4. Imputing numerical variables\n",
    "5. Classification with Decision Tree, single split\n",
    "6. Classification with Decision Tree, Cross validation\n",
    "7. Interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important instructions:\n",
    "\n",
    "Each function you make will be considered during the grading, so it is important to strictly follow input and output instructions stated in the skeleton code.\n",
    "\n",
    "You must not delete any of the given cells or change the structure of the cells or change the instructions in the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment. Should you contravene this provision, you will fail the assignment, and no feedback will be given on the part after the contravention.\n",
    "\n",
    "Some variable names are already given and have random values or empty arrays assigned on them. In this case you should only change the assignments on the variables but keep the names as given.\n",
    "\n",
    "When you are finished with implementing all the tasks, **clear all outputs, run all cells again** (make sure there is no error) and submit!\n",
    "\n",
    "Make sure that the results and figures asked are visible for us to grade.\n",
    "\n",
    "Make sure not to modify the files in the \"data\" folder in your submission, and not to change the folder structure or the files location, or your submission will not obtain a passing grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure consistent results, make sure that every operation in which you can use a random seed has it set to 8. If your process is correct, but the results are wrong due to the seed being wrong, it will be considered a major mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the libraries that you will need throughout the assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "RSEED = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.* Reading the files\n",
    "\n",
    "### `Task: Read the datasets using pandas. Use the files called cleveland.data and switzerland.data that you have downloaded in this archive.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contain information about adult patients from the US and from Switzerland. You can find more information in the heart-disease.names file in the 'data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the folder 'data', read the files cleveland.data and switzerland.data into the dataframes cleveland and test, respectively.\n",
    "# Make sure to add the names of the variables to both dataframes.\n",
    "\n",
    "columns = [] # you can find the column names in the file 'data/heart-disease.names'.\n",
    "# Select the correct column names for the dataset, as described in the file.\n",
    "\n",
    "cleveland = pd.DataFrame()  # change this\n",
    "test = pd.DataFrame()       # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "cleveland.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "# cleveland.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "# test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.* Missing values\n",
    "\n",
    "### `Task: Produce a plot with two subplots, each showing a bar plot of the 'missing' values (either encoded as NaN, or encoded with values that should not be in the dataset) for each feature for the two dataframes. The plot must have a name, and the bars must be named using the feature names.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 figure with a set of 2 subplots. Each axes should contain a figure as described below: \n",
    "# subplot 1: A barplot with the missing valies for each attribute in the dataset 'cleveland'\n",
    "# subplot 2: A barplot with the missing valies for each attribute in the dataset 'test'\n",
    "\n",
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3.* Imputing categorical variables\n",
    "\n",
    "In the file 'data/heart-disease.names' you can find, together with the names of the columns, a description of their contents.\n",
    "\n",
    "Determine which columns are categorical, and set their type to object.\n",
    "\n",
    "Determine which columns are numerical, and set their type accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []        # change this\n",
    "numerical_columns_int = []      # change this\n",
    "numerical_columns_float = []    # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Split the cleveland dataframe in a train and a validation set. `\n",
    "\n",
    "The train set must be called train, the the validation set must be called val. The size of the validation set must be 30% of the total size of the cleveland dataframe. Use shuffle=True and stratify=True. Make sure that both train and val are dataframes, and that the columns have the correct names. Reset the indexes of all four the dataframes, using drop=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y, where X contains the features and y contains the target variable.\n",
    "X_cleveland = pd.DataFrame()  # change this\n",
    "y_cleveland = pd.DataFrame()  # change this\n",
    "\n",
    "X_test = pd.DataFrame()       # change this\n",
    "y_test = pd.DataFrame()       # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "X_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the classification task easier, transform the target variable into a binary variable.\n",
    "# If the target variable is 0, it should remain 0. If the target variable is different from 0, it should be transformed into 1.\n",
    "y_train = pd.DataFrame()  # change this\n",
    "y_val = pd.DataFrame()    # change this\n",
    "y_test = pd.DataFrame()   # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "\n",
    "'''\n",
    "train_set_from_franco = pd.read_csv('../../../testing_data/train.csv')\n",
    "val_set_from_franco = pd.read_csv('../../../testing_data/val.csv')\n",
    "\n",
    "assert train.equals(train_set_from_franco), 'train set is not correct'\n",
    "assert val.equals(val_set_from_franco), 'validation set is not correct'\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: impute the missing values in the categorical columns. Use a KNNImputer from sklearn for the imputation process. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a subset of the train dataset with only the categorical columns. Call this subset train_cat.\n",
    "# 2. Create a subset of the val dataset with only the categorical columns. Call this subset val_cat.\n",
    "# 3. Create a subset of the test dataset with only the categorical columns. Call this subset test_cat\n",
    "# 4. Impute the three datasets using a KNN imputer with k=5 and weights set to distance\n",
    "# 5. Save the results in train_imputed_knn, val_imputed_knn, and test_imputed_knn.\n",
    "# 6. Make sure to add the column names to the resulting dataframes. DO NOT SKIP THIS STEP.\n",
    "# The new values might have new values that are not in the original dataset.\n",
    "# Approximate them to the nearest value in the original dataset, for each column.\n",
    "# To do so, you can store the original values of each column in a dictionary or a list.\n",
    "# if a new value is equidistant from two original values, choose the largest one.\n",
    "# (Example: if the original values are [1, 3] and the new value is 2, it will become 3)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "train_imputed_knn = pd.DataFrame()  # change this\n",
    "val_imputed_knn = pd.DataFrame()    # change this\n",
    "test_imputed_knn = pd.DataFrame()   # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "'''\n",
    "train_imp_knn_franco = pd.read_csv('../../../testing_data/train_imp_knn.csv')\n",
    "val_imp_knn_franco = pd.read_csv('../../../testing_data/val_imp_knn.csv')\n",
    "test_imp_knn_franco = pd.read_csv('../../../testing_data/test_imp_knn.csv')\n",
    "\n",
    "assert train_imputed_knn.equals(train_imp_knn_franco), 'train imputed knn is not correct'\n",
    "assert val_imputed_knn.equals(val_imp_knn_franco), 'val imputed knn is not correct'\n",
    "assert test_imputed_knn.equals(test_imp_knn_franco), 'test imputed knn is not correct'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4.* Imputing numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: impute the missing values in the numerical columns. Use a Lasso Regression from sklearn for the imputation process. `\n",
    "If more than one column contains missing values, proceed in increasing order: the lowest number of missing values first, then the second lowest, then the third ...\n",
    "\n",
    "Exclude the columns with missing values when fitting your regressor: only train on columns without missing values. After a column has been imputed, it can be used to fit the regressor in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a subset of the train dataset with only the numerical columns. Call this subset train_num.\n",
    "# 2. Create a subset of the val dataset with only the numerical columns. Call this subset val_num.\n",
    "# 3. Create a subset of the test dataset with only the numerical columns. Call this subset test_num.\n",
    "# 4a. Create a subset of train_num containing the rows with missing values. Call this subset train_num_missing.\n",
    "# 4b. Create a subset of train_num containing the rows without missing values. Call this subset train_num_not_missing.\n",
    "# 5a. Create a subset of val_num containing the rows with missing values. Call this subset val_num_missing.\n",
    "# 5b. Create a subset of val_num containing the rows without missing values. Call this subset val_num_not_missing.\n",
    "# 6a. Create a subset of test_num containing the rows with missing values. Call this subset test_num_missing.\n",
    "# 6b. Create a subset of test_num containing the rows without missing values. Call this subset test_num_not_missing.\n",
    "# 7. Using a Lasso regression, impute the missing values in train_num_missing, val_num_missing, and test_num_missing.\n",
    "# On what should the Lasso regression be trained?\n",
    "# 8. Repeat steps 1-7 until all the missing values are imputed.\n",
    "# 9. Save the results in train_num_imputed_lasso, val_num_imputed_lasso, and test_num_imputed_lasso.\n",
    "# 10. Concatenate the imputed subsets with the subsets that did not contain missing values.\n",
    "# 11. Save the resulting datasets in train_imputed_lasso, val_imputed_lasso, and test_imputed_lasso.\n",
    "# IMPORTANT: The order of the rows should be the same as in the original datasets.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "train_imputed_lasso = pd.DataFrame()  # change this\n",
    "val_imputed_lasso = pd.DataFrame()    # change this\n",
    "test_imputed_lasso = pd.DataFrame()   # change this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DELETE/CHANGE THIS CELL\n",
    "'''\n",
    "train_imp_lasso_franco = pd.read_csv('../../../testing_data/train_imp_lasso.csv')\n",
    "val_imp_lasso_franco = pd.read_csv('../../../testing_data/val_imp_lasso.csv')\n",
    "test_imp_lasso_franco = pd.read_csv('../../../testing_data/test_imp_lasso.csv')\n",
    "\n",
    "assert train_imputed_lasso.equals(train_imp_lasso_franco), 'train imputed lasso is not correct'\n",
    "assert val_imputed_lasso.equals(val_imp_lasso_franco), 'val imputed lasso is not correct'\n",
    "assert test_imputed_lasso.equals(test_imp_lasso_franco), 'test imputed lasso is not correct'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5.* Classification with Decision Tree, using a single split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the train_imputed_knn and train_imputed_lasso datasets. Call the resulting dataset X_train_imputed.\n",
    "# Merge the val_imputed_knn and val_imputed_lasso datasets. Call the resulting dataset X_val_imputed.\n",
    "# Merge the test_imputed_knn and test_imputed_lasso datasets. Call the resulting dataset X_test_imputed.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "X_train_imputed = pd.DataFrame()  # change this\n",
    "X_val_imputed = pd.DataFrame()    # change this\n",
    "X_test_imputed = pd.DataFrame()   # change this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Train a set of Decision Trees, using different hyperparameters. Use the best performing Decision Tree to predict the class for the test set. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a dictionary to contain the hyperparameters. The dictionary should contain the following:\n",
    "# - criterion: 'gini' and 'entropy'\n",
    "# - max_depth: 3, 5, and 7\n",
    "# - min_samples_split: 2, 5, and 10\n",
    "# 2. Create a dictionary called performance to store the hyperparameter combinations and the corresponding performance of the model.\n",
    "# 3. Create a ParameterGrid object with the hyperparameters from the dictionary.\n",
    "# 4. Create a for loop to iterate over the combinations of hyperparameters.\n",
    "# 5. In each iteration\n",
    "# - Create a DecisionTreeClassifier with the hyperparameters for that iteration.\n",
    "# - Fit the model.\n",
    "# - Predict the target variable for the validation set.\n",
    "# - Calculate the F1 score of the model.\n",
    "# - Add the hyperparameters and the F1 score to the performance dictionary.\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "hyperparameters = {} # change this\n",
    "# add missing steps here\n",
    "\n",
    "start = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "for number in range(1, 11): # change this\n",
    "    # Add the missing steps here.\n",
    "    pass # remove this line\n",
    "\n",
    "end = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with a single split: ', end - start) # DO NOT CHANGE/DELETE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best performing hyperparameters\n",
    "best_hyperparameters = None # change this\n",
    "\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train and validation datasets. Call the resulting datasets X and y.\n",
    "X = pd.DataFrame()  # change this\n",
    "y = pd.DataFrame()  # change this\n",
    "\n",
    "# Create a DecisionTreeClassifier with the best hyperparameters.\n",
    "# Fit the model on the X and y datasets.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "# Predict the target variable for the test dataset.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "# Calculate the F1 score of the model on the test dataset. Call the variable f1_test_single_split.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "f1_test_single_split = None # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test_single_split # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6.* Classification with Decision Tree using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Train a cross-validation object, then train a decision tree using cross-validation and different hyperparameters. Use the best performing Decision Tree to predict the class for the test set. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use the same hyperparameters from the previous task.\n",
    "# 2. Create a StratifiedKFold object with 5 splits, use shuffle=True.\n",
    "# 3. Create a dictionary called performance_CV to store the hyperparameter combinations and the corresponding performance of the model.\n",
    "# 3. Create a ParameterGrid object with the usual hyperparameters.\n",
    "# 4. Create a for loop to iterate over the folds of the StratifiedKFold.\n",
    "# 5. For each fold, create a for loop to iterate over the combinations of hyperparameters.\n",
    "# 6. In each iteration\n",
    "# - Create a DecisionTreeClassifier with the hyperparameters for that iteration.\n",
    "# - Fit the model.\n",
    "# - Predict the target variable for the validation fold.\n",
    "# - Calculate the F1 score of the model.\n",
    "# - Add the hyperparameters and the F1 score to the performance dictionary. Each hyperparameter combination may have multiple F1 scores.\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = [[5,6], [10,11], [15,16], [20,21], [25,26], [30,31], [35,36], [40,41], [45,46], [50,51]]    # Delete this line\n",
    "y = [0,1,0,1,0,1,0,1,0,1]                                                                       # Delete this line\n",
    "\n",
    "X = pd.DataFrame(X)                                                                             # Delete this line\n",
    "y = pd.DataFrame(y)                                                                             # Delete this line\n",
    "\n",
    "\n",
    "# DO NOT FORGET TO DELETE THE PREVIOUS LINES. They are only to make the empty assignment run without errors,\n",
    "# but they will destroy the data you need.\n",
    "\n",
    "\n",
    "CV = StratifiedKFold() # change this\n",
    "\n",
    "hyperparameters = None # change this\n",
    "# add missing steps here\n",
    "\n",
    "start_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(CV.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    for number in range(1, 11): # change this\n",
    "        # Add the missing steps here.\n",
    "        pass # remove this line\n",
    "\n",
    "end_CV = time.time() # DO NOT CHANGE/DELETE THIS LINE\n",
    "\n",
    "print('Time elapsed to run the hyperparameter tuning with Cross Validation: ', end_CV - start_CV) # DO NOT CHANGE/DELETE THIS LINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the best performing hyperparameters, which are the ones with the highest average F1 score\n",
    "best_hyperparameters_CV = None # change this\n",
    "\n",
    "best_hyperparameters_CV # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DecisionTreeClassifier with the best hyperparameters.\n",
    "# Fit the model on the X and y datasets.\n",
    "# Call the fitted model final_tree.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "# Predict the target variable for the test dataset.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "# Calculate the F1 score of the model on the test dataset. Call the variable f1_test_CV.\n",
    "\n",
    "# Write your code here\n",
    "\n",
    "f1_test_CV = None # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test_CV # DO NOT DELETE/CHANGE THIS LINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *7.* Interpretation of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Look at the times elapsed to train the Decision Tree using the single split and the CV strategies. Is there a difference? Explain the difference or the lack of difference in 50 words or less. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your explanation here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ` Task: Plot final_tree, and explain which feature or combination of features is the most relevant for that model, in 50 words or less. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find instructions on how to plot a decision tree at [this link](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your tree here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your explanation here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
