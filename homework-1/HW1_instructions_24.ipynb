{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Data exploration and Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will explore the dataset, handle the missing values, standardize the numerical values and reduce the dimensionality of the feature space. The learning outcome of this part is to know how one can pre-process a real-world dataset and prepare for an supervised or unsupervised learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student information\n",
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Pass/Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE: \n",
    "\n",
    "Data pre-processing, plotting and dimensionality reduction\n",
    "\n",
    "1. Reading the file\n",
    "2. Missing Values\n",
    "3. Impute with scikit-learn\n",
    "4. Implement imputation\n",
    "5. Plotting\n",
    "6. Standardization\n",
    "7. Dimensionality reduction\n",
    "8. Multi-Dimensional Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "\n",
    "Each function you make will be considered during the grading, so it is important to strictly follow input and output instructions stated in the skeleton code.\n",
    "\n",
    "You should not delete any of the given cells or change the structure of the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment.\n",
    "\n",
    "Some variable names are already given and have random values or empty arrays assigned on them. In this case you should only change the assignments on the variables but keep the names as given.\n",
    "\n",
    "When you are finished with implementing all the tasks, **clear all outputs, run all cells again** (make sure there is no error) and submit!\n",
    "\n",
    "Make sure that the results and figures asked are visible for us to grade. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the libraries that you will need throughout the assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "RSEED = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PRE-PROCESSING, PLOTTING AND DIMENSIONALITY REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Pima Indians Diabetes Database** that is publicly available and from UCI. However, we removed and changed some parts of the dataset for the homework evaluation, so **please use the one in the zip file, not the original one**.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on specific diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of several medical predictors (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "According to the information on the data, it has eight attributes and one binary class. The brief explanation of the attributes are as follows:\n",
    "\n",
    "- Pregnancies: Number of times pregnant.\n",
    "\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg).\n",
    "\n",
    "- SkinThickness: Triceps skin fold thickness (mm).\n",
    "\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function.\n",
    "\n",
    "- Age: Age (years).\n",
    "\n",
    "- and we have a binary class which can be 0 (healthy) or 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *1.* Reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Read the dataset using pandas. Use the csv file called diabetes.csv that you will find on ilearn.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found at dataset/diabetes.csv\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# open the diabetes csv file in a function\n",
    "\n",
    "def open_diabetes_csv(file_path):   \n",
    "  try:\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "  except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    return None\n",
    "\n",
    "diabetes_df = open_diabetes_csv('dataset/diabetes.csv')\n",
    "data = diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do not delete this!\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "# if you want to see information about the dataset:\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot describe a DataFrame without columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if you want to see information about the dataset, uncomment:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/generic.py:11976\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[0;34m(self, percentiles, include, exclude)\u001b[0m\n\u001b[1;32m  11734\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m  11735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[1;32m  11736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11739\u001b[0m     exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  11740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m  11741\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  11742\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[1;32m  11743\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11974\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[1;32m  11975\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 11976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdescribe_ndframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11978\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpercentiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpercentiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/methods/describe.py:91\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[0;34m(obj, include, exclude, percentiles)\u001b[0m\n\u001b[1;32m     87\u001b[0m     describer \u001b[38;5;241m=\u001b[39m SeriesDescriber(\n\u001b[1;32m     88\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     describer \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrameDescriber\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataFrame\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/core/methods/describe.py:162\u001b[0m, in \u001b[0;36mDataFrameDescriber.__init__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude \u001b[38;5;241m=\u001b[39m exclude\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot describe a DataFrame without columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(obj)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot describe a DataFrame without columns"
     ]
    }
   ],
   "source": [
    "# if you want to see information about the dataset, uncomment:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *2.* Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems like there is no null data encoded as such. However, if you check the values in the dataset, there are many that might look strange (maybe some values that do not make sense?). \n",
    "\n",
    "### `Task: Plot a bar plot of the 'missing' values (values that are encoded with values that do not make sense) per attribute, excluding the attributes 'Pregnancies', 'Outcome'. The plot must have a title and the bars of the plot must be named to their respective attribute names.` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the steps are just indicative, if you want to do it your own way, please do so as long as you print the required barplot, specified above \n",
    "# in the task description. \n",
    "# The style, color, or library used for plotting will not affect the grading of the task (or any of the tasks that require plots)\n",
    "\n",
    "\n",
    "# step 1: store the sum of missing values per attribure (excluding the attributes 'Pregnancies', 'Outcome') in a pandas Series\n",
    "# step 2: plot the missing values series as a barplot, using the plot function from pandas (or any other library, as you prefer). \n",
    "\n",
    "# Write your code here\n",
    "\n",
    "def identify_missing_data(data):\n",
    "\n",
    "    # Identifying the missing values by looking for empty cells or 0s\n",
    "    missing_values = data[(data == 0) | (data == -1)].count()\n",
    "    \n",
    "    # Excluding 'Pregnancies' and 'Outcome', getting attributes\n",
    "    att_to_plot = [col for col in data.columns if col not in ['Pregnancies', 'Outcome']] # check if the columns is not Pregnancy or outcome\n",
    "    # store missing values in a pandas Series\n",
    "    missing_values_on_att = pd.Series(missing_values[att_to_plot])\n",
    "    # Filter out attributes with 0 missing values\n",
    "    missing_values_filtered = missing_values_on_att[missing_values_on_att > 0]\n",
    "\n",
    "    # print(missing_values_on_att)\n",
    "    return missing_values_on_att, missing_values_filtered\n",
    "\n",
    "missing_values = identify_missing_data(data)\n",
    "\n",
    "\n",
    "# part 2.2 Create the barplot\n",
    "def create_bar_plot(values):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(values.index, \n",
    "            values.values)\n",
    "    # print(values.index)\n",
    "    plt.title('Missing Values bar plot')    # Plot title\n",
    "    plt.ylabel('Missing Values count')      # Plot Y\n",
    "    plt.xlabel('Attributes name')           # Plot X\n",
    "    plt.xticks(rotation=45, ha='right')     \n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "plot = create_bar_plot(missing_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *3.* Impute with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it normal that people's BMI is so low, in some cases? so low that they seem to not have a mass? or not? What about Glucose, and Blood Pressure? You may want to change some values into another reasonable value, such as mean or median. The only thing that can have zero value is the attribute **'Pregnancies'**. \n",
    "\n",
    "### **NOTE**! The attribute **'Outcome'** is your class. The class SHOULD NOT be included the cleaning process.\n",
    "\n",
    "### `Task: Impute the missing values using the SimpleImputer from scikit-learn with strategy = 'mean'. Return a pandas dataframe called 'data_imputed' that includes both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). The dataframe called 'data' SHOULD REMAIN UNCHANGED (not imputed) as it will be used again in the next task.`\n",
    "### For the scikit-learn imputation, you can find more information [here](https://scikit-learn.org/stable/modules/impute.html), or in the recorded lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scickit-learn version\n",
    "# The steps are just indicative, if you want to do it your own way, please do so as long you store in data_imputed a dataset that includes \n",
    "# both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). The dataframe 'data' must remain unchanged (not imputed). \n",
    "  \n",
    "# step 1: create a copy of the original dataset called data_imputed\n",
    "# step 2: create a list with the names of the attributes that you will impute, call it columns_to_impute\n",
    "# step 3: create a new dataset called df_part, that includes only the columns that you will impute (the ones from step 2)\n",
    "# step 4: define the SimpleImputer object (from sklearn) with strategy='mean' and call it imputer, fit and transform the dataset that you created in step 3 \n",
    "# step 5: convert the resulting array from step 4 into a dataframe, as column names you can pass the names of attributes of the list that you created in step 2. call it df_converted\n",
    "# step 6: in data_imputed (which is still a copy of data (the original dataset) replace the attributes you wanted to impute with the attribures in df_converted. The dataframe called 'data' should remain \n",
    "# unchanged. \n",
    "# data_imputed should contain all 8 attributes and the class, where every attribute -except 'Pregnancies' and 'Outcome'- have imputed values \n",
    " \n",
    "\n",
    "\n",
    "# Write your code here\n",
    "# Create a copy of the original DataFrame\n",
    "data_imputed = data.copy() #1 \n",
    "\n",
    "# Identify columns with potentially missing values (excluding 'Pregnancies' and 'Outcome')\n",
    "columns_to_impute = [col for col in data.columns if col not in ['Pregnancies', 'Outcome'] and (data[data[col] == 0].shape[0] < data.shape[0])]# 2\n",
    "\n",
    "df_part = data[columns_to_impute] #3\n",
    "\n",
    "# Create a SimpleImputer with strategy='mean'\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the imputer on the selected DataFrame\n",
    "df_part_imputed = imputer.fit_transform(df_part) # 4\n",
    " \n",
    "df_imputed = pd.DataFrame(df_part_imputed, columns=columns_to_impute) # 5\n",
    "\n",
    "# Fit and transform the imputer on the selected columns\n",
    "data_imputed[columns_to_impute] = df_part.fit_transform(data_imputed[columns_to_impute])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "data_imputed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation for task3, uncomment the following:\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *4.* Implement imputation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Write a function that changes the zero (or nan) value to the mean of the attribute of a dataframe. `\n",
    "\n",
    "### You will impute the missing values with the mean of the column without using scikit-learn. You will store the resulting dataset in diabetes_1, that includes both the imputed attributes and the ones that you did not impute (Pregnancies, Outcome). diabetes_1 will not be used again after this task. \n",
    "\n",
    "Note: **Do not** use sklearn here. The goal is for you to write your *own* imputation function, and to use it. Since you **will not** use sklearn, there is no need to convert the imputed dataframe to a dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(df, columns_to_impute):\n",
    "        \n",
    "    df_imputed = df.copy()  # Create a copy\n",
    "    \n",
    "    # Replace 0 with NaN \n",
    "    df_imputed.replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # Iterate through the specified columns and replace NaN with the column mean\n",
    "    for col in columns_to_impute:\n",
    "        mean = df_imputed[col].mean()\n",
    "        df_imputed[col].fillna(mean, inplace=True)\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "imputation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "diabetes_1 = imputation(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "\n",
    "try:\n",
    "    np.testing.assert_allclose(data_imputed.values, diabetes_1.values)\n",
    "    print(\"result: equal\")\n",
    "except:\n",
    "    print(\"result: not equal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "diabetes_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you successfuly finished the imputation, uncomment this\n",
    "data_imputed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *5.* Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. In this task we will explore the attribute 'Glucose' with plotting. \n",
    "\n",
    "### `Task: Create 1 figure with a set of 3 subplots. We have seen something very similar in the lab, with 4 subplots. `\n",
    "\n",
    "\n",
    "#### - `In the 1st subplot create a boxplot for the attribute Glucose.`\n",
    "\n",
    "\n",
    "#### - `In the 2nd subplot create a histogram for the attribute Glucose.`\n",
    "\n",
    "\n",
    "#### - `In the 3rd subplot create a scatterplot for the attributes Glucose and BloodPressure, colored by the attribute 'Outcome'(add a legend to identify color - class label pairs).`\n",
    "`The plots should have titles, the figsize should be big enough (for example figsize=(16,9)). If you cannot create any of the required plots either remove the respective axis and make for example  1 figure with 2 subplots or create the figure with three subplots and leave the respective axis empty.'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1 figure with a set of 3 subplots. Each axes should contain a figure as described below: \n",
    "# Figure 1: A boxplot for the attribure 'Glucose'\n",
    "# Figure 2: A histogram for the attribure 'Glucose'\n",
    "# Figure 3: A scatterplot of Glucose vs BloodPressure, colored by the attribure 'Outcome', include a legend to identify which class-color pairs\n",
    "\n",
    "# All subplots must have a title that makes sense.\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16,9)) #create 1 row with 3 plots\n",
    "#Write your code here\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 9)) \n",
    "\n",
    "# Subplot 1: Boxplot of Glucose\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.boxplot(data['Glucose'])\n",
    "plt.ylabel('Glucose Level')\n",
    "plt.title('Boxplot of Glucose')\n",
    "\n",
    "# Subplot 2: Histogram of Glucose\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(data['Glucose'], bins=20)\n",
    "plt.xlabel('Glucose Level')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Glucose')\n",
    "\n",
    "# Subplot 3: Scatter plot of Glucose vs. BloodPressure colored by Outcome\n",
    "plt.subplot(1, 3, 3)\n",
    "scatter = plt.scatter(data['Glucose'], data['BloodPressure'], c=data['Outcome'])\n",
    "plt.xlabel('Glucose Level')\n",
    "plt.ylabel('Blood Pressure')\n",
    "plt.title('Glucose Level vs. Blood Pressure (Colored by Outcome)')\n",
    "\n",
    "# Add a legend to identify color-class label pairs\n",
    "plt.legend(*scatter.legend_elements(), title=\"Outcome\")\n",
    "\n",
    "plt.tight_layout()  # Adjust subplot spacing\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.\tAge by class \n",
    "\n",
    "### `Task:  Plot the 'Age' attribute in groups of 10 years in relation to the class ('Outcome').`\n",
    "\n",
    "Information about the cut function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Divide the age column into the following age groups: 0-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-100 using the cut function from pandas. \n",
    "# The new column should be called age_bins.\n",
    "# step 2: Store in age_by_class, a dataframe with the counts of unique values of the attribute age_bins by class. You can use groupby() and value_counts()\n",
    "# step 2: Plot a barplot for the age_by_class. You can use pandas to plot it.\n",
    "# step 3: **DROP THE age_bins** attribute from data_imputed after you have plotted the barchart. \n",
    "# do not skip dropping the age_bins!\n",
    "\n",
    "# Create age groups\n",
    "bins = [0, 19, 29, 39, 49, 59, 69, 79, 100]\n",
    "labels = ['0-19', '20-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-100']\n",
    "data['age_bins'] = pd.cut(data['Age'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Step 2: \n",
    "age_by_class = data.groupby('Outcome')['age_bins'].value_counts().unstack()\n",
    "\n",
    "# dropping the age_bins!\n",
    "data_imputed = data_imputed.drop('age_bins', axis=1)\n",
    "# Plot \n",
    "ax = age_outcome_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Age Groups vs Outcome')\n",
    "plt.xticks(ha='right')\n",
    "plt.legend(title='Outcome', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "data_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.\tYour own multivariate question\n",
    "\n",
    "### `Task: think of a multivariate question that can be answered with a plot. Write it in the markdown cell below this. Write the code to create the plot in the code cell below. Write a short answer to your question (max 50 words) in the markdown cell after that.`\n",
    "\n",
    "Bonus points (only in how much I'll enjoy the answer, since there are no points in this homework) if your question is original, so please don't make me read 300 copies of the same question :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your multivariate question here. Delete this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a  plot with Glucose, BMI, and Age as axes and color-coded by Outcome\n",
    "sns.scatterplot(x='Glucose', y='BMI', hue='Outcome', size='Age', data=data_imputed)\n",
    "plt.title('Glucose vs. BMI vs. Age (Colored by Outcome)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your short answer (max 50 words) here. Delete this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *6.* Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization transforms data to have a mean of zero and a standard deviation of 1. \n",
    "\n",
    "### It is a crucial step before performing PCA, since we are interested in the components that maximize the variance. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### `Task: Standardize the data_imputed dataset. You can use sklearn.  Store in a variable called 'y' the attribute 'Outcome' (your class)`\n",
    "### NOTE! Outcome is the class of the dataset indicating if a patient is healthy or has diabetes. As we discussed in the lab, the class should not be included in the standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: Use StandardScaler to fit_transform data_imputed, excluding the class (Outcome)\n",
    "# step 3: Transform the standardized numpy matrix returned by StandardScaler into a dataframe called data_standardized.\n",
    "# step 4: Rename the columns of the dataframe with their corresponding names.\n",
    "# step 5: Store in a variable called y the attribute Outcome (your class), do not skip this\n",
    "\n",
    "features_to_standardize = [col for col in data_imputed.columns if col != 'Outcome'] # 1\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data for the selected features\n",
    "data_standardized = scaler.fit_transform(data_imputed[features_to_standardize])\n",
    "\n",
    "# Convert the standardized data back to a DataFrame\n",
    "data_standardized_df = pd.DataFrame(data_standardized, columns=features_to_standardize)\n",
    "\n",
    "# Adding 'Outcome' \n",
    "data_standardized_df['Outcome'] = data_imputed['Outcome']\n",
    "\n",
    "y = data_standardized_df['Outcome']\n",
    "\n",
    "# rename columns\n",
    "new_column_names = [col for col in data_imputed.columns if col != 'Outcome']\n",
    "data_standardized.columns = new_column_names\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "data_standardized.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *7.* Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. PCA\n",
    "\n",
    "### `Task: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component Analysis. Print the information obtained by the following attributes of the pca object: explained_variance_ratio and components_. `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: Reduce the dimensionality of the standardized dataset to 2 Principal Components, with Principal Component analysis. You can use PCA from sklearn. Use random state = 8\n",
    "# step2: Store the explained variance ratio in an array called explained_variance_ratio.\n",
    "# step3: Store in a dataframe called df_principal_components, the result of pca's attribute components_,\n",
    "# (Principal axes in feature space, representing the directions of maximum variance in the data), with the respective attribute names.\n",
    "\n",
    "# create PCA\n",
    "pca = PCA(n_components=2, random_state=8)\n",
    "\n",
    "principal_components = pca.fit_transform(data_standardized)\n",
    "\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "principal_df['Outcome'] = y\n",
    "\n",
    "\n",
    "explained_variance_ratio =  pca.explained_variance_ratio_\n",
    "df_principal_components = pd.DataFrame(pca.components_, columns=data_standardized.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explained_variance_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. \tWhich attribute contributes the most?\n",
    "\n",
    "### `Task: Store in a variable called attribure_contributing_the_most, which attribure contributes most to the variance of the 1st PC.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which attribute contributes the most in the variance of the 1st principal component? \n",
    "# store the name of the attribute here, as a string type:\n",
    "\n",
    "pc1_loadings = df_principal_components.iloc[0]\n",
    "\n",
    "# BMI\n",
    "attribute_contributing_the_most =  c1_loadings.abs().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this\n",
    "attribute_contributing_the_most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  Multi-Dimensional Scaling\n",
    "\n",
    "Multidimensional scaling (MDS) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.\n",
    "In general, MDS is a technique used for analyzing similarity or dissimilarity data and it can help visualize the distances or dissimilarities between sets of objects. Examples of similarity or dissimilarity data might include the distance between pairs of cities, or planets at a particular point in time, or the similarity among groups of people (voters, patients etc). \n",
    "\n",
    "In these last two excercises we will apply Multi-Dimensional Scaling in our patient dataset using two different versions of the MDS sklearn algorithm. \n",
    "\n",
    "We will focus on the attribute **dissimilarity** of the MDS object. The attribute can be either 'euclidean' or 'precomputed'. In the former case the euclidean distance between the data points is computed by the algorithm, while in the latter case the user must themeselves compute the dissimilarities between data points and pass this to fit_transform.  \n",
    "\n",
    "Please check the sklearn page for MDS to be able to implement the above tasks: [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html). Make sure you understand the parameters of fit_transform and how you could use them for the tasks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Multi-Dimensional Scaling, task a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   `Task: Apply MDS on the data_standardized with n_components=2 and dissimilarity='euclidean'. Plot the 2 resulting coordinates (in a scatterplot) with colors respective to the class labels.`\n",
    "\n",
    "Note: This is very similar to how we applied sklearn's PCA on task 7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # step 1: Initialize MDS with n_components = 2, random_state=8.\n",
    "# step 2: Fit and transform the standardized dataset\n",
    "# step 3: Plot the resulting reduced dataset with colors respective to the class (Outcome).\n",
    "\n",
    "# Step 1: Initialize MDS\n",
    "mds = MDS(n_components=2, dissimilarity='euclidean', random_state=8)\n",
    "mds_coordinates = mds.fit_transform(data_standardized)\n",
    "\n",
    "mds_df = pd.DataFrame(data=mds_coordinates, columns=['MDS1', 'MDS2'])\n",
    "mds_df['Outcome'] = y\n",
    "\n",
    "# Step 3: Plot the resulting reduced dataset with colors respective to the class (Outcome)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mds_df['MDS1'], mds_df['MDS2'], c=mds_df['Outcome'], cmap='viridis')\n",
    "plt.xlabel('MDS1')\n",
    "plt.ylabel('MDS2')\n",
    "plt.title('MDS Scatter Plot')\n",
    "plt.colorbar(label='Outcome')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mds_coordinates = mds.fit_transform(data_standardized)\n",
    "\n",
    "X_2d_a = np.zeros([2, 2]) # change this assignment on X_2d_a, but keep X_2d_a as the name of the resulting MDS numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2d_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code.  You should not delete any of the given cells or change the structure of the cells or add cells (unless completely necessary, add a comment on why you added a cell) as they will help in grading the assignment. Some variable names are already given and have random values or empty arrays assigned to them. In this case you should only change the assignments on the variables but keep the names as given. When you are finished with implementing all the tasks, clear all outputs, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for the grading. ` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Multi-Dimensional Scaling, task b (not graded, not part of the assignment, you can avoid doing it if you don't want to).\n",
    "\n",
    "There are no bonus points for this exercise. You can see it as a personal challenge if you want to do more than the basic and test your knowledge, it will not be graded (but I will go over it if you attempt it).\n",
    "\n",
    "Don't let it distract you from the rest of the lab, which you actually need to pass. You have nothing to gain from this exercise, and nothing to lose (apart from the time that you'll use to complete the exercise).\n",
    "\n",
    "Feel free to submit an incomplete solution if you want to try, but still don't manage to get a complete one, and you will get feedback on the missing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Compute the pairwise distances between observations using the euclidean metric. Apply MDS on the custom similarity matrix with n_components=2. Plot the results with colors respective to the class label. `\n",
    "\n",
    "Note: the resulting plot should look similar with the one of Task 8a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: compute the pairwise distances between observations using the euclidean metric. \n",
    "\n",
    "# One of the ways to do this is to use pdist and squareform from the scipy library (see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)) \n",
    "# or euclidean_distances from sklearn. In any case the returned array should be: distances ndarray of shape (n_samples_data_standardized, n_samples_data_standardized)\n",
    "\n",
    "# step 2: create an MDS object with n_components=2, random_state=8. What should the dissimilarity parameter be in this case?\n",
    "# step 3: apply MDS on the constructed square distance matrix from step 1\n",
    "# step 4: plot the results in a scatterplot with colors respective to the class label\n",
    "\n",
    "# Step 2: Create an MDS object\n",
    "mds = MDS(n_components=2, random_state=8, dissimilarity='precomputed')  \n",
    "\n",
    "\n",
    "embedding = mds.fit_transform(distance_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mds_df['MDS1'], mds_df['MDS2'], c=mds_df['Outcome'], cmap='viridis')\n",
    "plt.xlabel('MDS1')\n",
    "plt.ylabel('MDS2')\n",
    "plt.title('MDS Scatter Plot')\n",
    "plt.colorbar(label='Outcome')\n",
    "plt.show()\n",
    "\n",
    "X_2d_b = np.zeros([2, 2]) # change this assignment on X_2d_b, but keep X_2d_b as the name of the resulting MDS numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this \n",
    "try:\n",
    "    np.testing.assert_allclose(X_2d_b, X_2d_a, atol=0.00001)\n",
    "    print(\"result: equal\")\n",
    "except:\n",
    "    print(\"result: not equal\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d419f0c7497595e68d237e2dc8f4eac5bf2e06b53acfc5f80d3651d7a9e90a8a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
